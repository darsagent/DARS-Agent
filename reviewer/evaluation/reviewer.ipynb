{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "import asyncio\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from asyncio import Semaphore\n",
    "from litellm import completion\n",
    "from tqdm.notebook import tqdm\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any\n",
    "from pydantic import BaseModel, Field\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from utils import get_all_accepted_paths, get_all_rejected_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = ''\n",
    "os.environ['DEEPSEEK_API_KEY'] = ''\n",
    "os.environ['GEMINI_API_KEY'] = ''\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = ''\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = ''\n",
    "os.environ['AWS_DEFAULT_REGION'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add root files and evaluation files paths\n",
    "traj_files = sorted(['/path/to/root1', '/path/to/root2', '...'])\n",
    "eval_files = sorted(['/path/to/eval1', '/path/to/eval2', '...'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_obs = {}\n",
    "for traj_file, eval_file in zip(traj_files, eval_files):\n",
    "    issue1 = traj_file.split('/')[-2]\n",
    "    issue2 = eval_file.split('/')[-2]\n",
    "    if issue1 != issue2:\n",
    "        print(issue1, issue2)\n",
    "    assert issue1 == issue2\n",
    "    issue = issue1\n",
    "    with open(traj_file, 'r') as f:\n",
    "        traj = json.load(f)\n",
    "    with open(eval_file, 'r') as f:\n",
    "        eval_dict = json.load(f)\n",
    "    all_accepted_paths = get_all_accepted_paths(traj['root'], eval_dict)\n",
    "    all_rejected_paths = get_all_rejected_paths(traj['root'], eval_dict, issue)\n",
    "    act_obs[issue] = []\n",
    "    for path in all_accepted_paths:\n",
    "        ac_ob = []\n",
    "        for i, node in enumerate(path):\n",
    "            if node['role'] == 'assistant':\n",
    "                ac_ob.append((node['action'], path[i+1]['content'], node['thought']))\n",
    "        act_obs[issue].append((ac_ob, 1))\n",
    "    for path in all_rejected_paths:\n",
    "        ac_ob = []\n",
    "        for i, node in enumerate(path):\n",
    "            if node['role'] == 'assistant':\n",
    "                ac_ob.append((node['action'], path[i+1]['content'], node['thought']))\n",
    "        act_obs[issue].append((ac_ob, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context from trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_trajectories(act_obs):\n",
    "    new_act_obs = []\n",
    "    for ac_ob, status in act_obs:\n",
    "        new_ac_ob = []\n",
    "        for act, obs, thought in ac_ob:\n",
    "            if not act:\n",
    "                continue\n",
    "            action = act.split()[0]\n",
    "            if 'submit' in action: \n",
    "                action = 'submit'\n",
    "            if action not in ['append', 'create', 'edit', 'undo_edit', 'submit', 'insert', 'execute_ipython', 'execute_server', 'django-admin', 'pylint', 'pytest', 'python', 'python2', 'python3', 'sphinx-build', 'tox']:\n",
    "                continue\n",
    "            if action in ['execute_ipython', 'execute_server']:\n",
    "                if \"Error: Server script \" in obs or \"Error starting server process:\" in obs or \"Error connecting to server:\" or \"No response from server.\" in obs or \"Error: 'code' must not be empty.\" in obs or \"Usage: execute_ipython $<code>\" or \"Error: Server script\" in obs or \"Error: Server did not start within expected time.\" in obs or \"Server not running, starting server...\" in obs or \"Error connecting to server after restart:\" in obs or \"Error connecting to server:\" in obs or \"No response from server.\" in obs or \"Error during communication with server:\" in obs:\n",
    "                    continue\n",
    "            if action in ['python', 'python2', 'python3']:\n",
    "                if 'No such file or directory\\n(Open file:' in obs:\n",
    "                    continue\n",
    "            if action in ['edit', 'append', 'insert'] and 'File updated ' not in obs:\n",
    "                continue\n",
    "            if action == 'create' and ('Usage: create ' in obs or 'Error: File ' in obs):\n",
    "                continue\n",
    "            if obs.startswith('/bin/bash:'):\n",
    "                continue\n",
    "            if '\\nEXECUTION TIMED OUT. If the command is intended to run in the background or requires more time to complete' in obs:\n",
    "                continue\n",
    "            if obs.startswith('/root/commands/defaults.sh'):\n",
    "                continue\n",
    "            if action == 'execute_ipython' and \"Error: 'code' must not be empty.\" in obs:\n",
    "                continue\n",
    "            if action == 'execute_server' and ('Error starting server process:' in obs or 'Error: Server' in obs or 'Error connecting to server' in obs or 'Error during communication with server' in obs):\n",
    "                continue\n",
    "            if action == 'undo_edit' and 'Successfully restored ' not in obs:\n",
    "                continue\n",
    "            \n",
    "            new_ac_ob.append((act, obs, thought))\n",
    "        new_act_obs.append((new_ac_ob, status))\n",
    "\n",
    "    return new_act_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_len = 0\n",
    "total_paths = 0\n",
    "for issue in act_obs:\n",
    "    total_paths += len(act_obs[issue])\n",
    "    for ac_ob, _ in act_obs[issue]:\n",
    "        avg_len += len(ac_ob)\n",
    "avg_len / total_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_act_obs = {}\n",
    "for issue in act_obs:\n",
    "    new_act_obs[issue] = clean_trajectories(act_obs[issue])\n",
    "avg_len = 0\n",
    "total_paths = 0\n",
    "for issue in new_act_obs:\n",
    "    total_paths += len(new_act_obs[issue])\n",
    "    for ac_ob, _ in new_act_obs[issue]:\n",
    "        avg_len += len(ac_ob)\n",
    "round(avg_len / total_paths, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_paths = 0\n",
    "avg_paths_resolved = 0\n",
    "total_resolved = 0\n",
    "success_ratio = []\n",
    "for issue in new_act_obs:\n",
    "    avg_paths += len(new_act_obs[issue])\n",
    "    success = 0\n",
    "    for path, status in new_act_obs[issue]:\n",
    "        if status == 1:\n",
    "            success += 1\n",
    "    if success != 0:\n",
    "        success_ratio.append(success / len(new_act_obs[issue])) \n",
    "        avg_paths_resolved += len(new_act_obs[issue])\n",
    "        total_resolved += 1\n",
    "round(avg_paths / len(new_act_obs), 1), round(sum(success_ratio) / len(success_ratio), 2), round(avg_paths_resolved / total_resolved, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_trajectories(new_act_obs):\n",
    "    all_unique_trajs = {}\n",
    "    \n",
    "    for issue in tqdm(new_act_obs):\n",
    "        # Dictionary to track shortest trajectory for each patch in this issue\n",
    "        patch_to_traj = {}\n",
    "        \n",
    "        # Sort trajectories by length\n",
    "        sorted_trajs = sorted(new_act_obs[issue], key=lambda x: len(x[0]))\n",
    "        \n",
    "        # Process each trajectory\n",
    "        for traj in sorted_trajs:\n",
    "            action_obs = traj[0]\n",
    "            patch = action_obs[-1][1].split('(Open file:')[0]\n",
    "            if patch not in patch_to_traj:\n",
    "                patch_to_traj[patch] = traj\n",
    "        \n",
    "        # Store unique trajectories for this issue\n",
    "        all_unique_trajs[issue] = list(patch_to_traj.values())\n",
    "    \n",
    "    return all_unique_trajs\n",
    "\n",
    "all_unique_trajs = get_unique_trajectories(new_act_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_paths = 0\n",
    "avg_paths_resolved = 0\n",
    "total_resolved = 0\n",
    "success_ratio = []\n",
    "for issue in all_unique_trajs:\n",
    "    avg_paths += len(all_unique_trajs[issue])\n",
    "    success = 0\n",
    "    for path, status in all_unique_trajs[issue]:\n",
    "        if status == 1:\n",
    "            success += 1\n",
    "    if success != 0:\n",
    "        success_ratio.append(success / len(all_unique_trajs[issue]))\n",
    "        avg_paths_resolved += len(all_unique_trajs[issue])\n",
    "        total_resolved += 1\n",
    "\n",
    "round(avg_paths / len(all_unique_trajs), 1), round(sum(success_ratio) / len(success_ratio), 2), round(avg_paths_resolved / total_resolved, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproduction Script & Main Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_created_files(traj):\n",
    "    created_files = []\n",
    "    for act, obs, thought in traj:\n",
    "        action = act.split()[0]\n",
    "        if action == 'create':\n",
    "            created_files.append(obs.split('Open file: ')[-1].split(')\\n')[0])\n",
    "    return created_files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_edits_files(traj):\n",
    "    edited_files = []\n",
    "    for idx, (act, obs, _) in enumerate(traj):\n",
    "        action = act.split()[0]\n",
    "        if action == 'edit' or action == 'append' or action == 'insert':\n",
    "            edited_files.append((obs, idx))\n",
    "    return edited_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'^\\[File:.*?\\([0-9]+\\s+lines\\s+total.*?\\)]\\s*'\n",
    "all_created_files = {}\n",
    "last_edit_index = {}\n",
    "main_edited_files = {}\n",
    "for issue in all_unique_trajs:\n",
    "    all_created_files[issue] = []\n",
    "    last_edit_index[issue] = []\n",
    "    main_edited_files[issue] = []\n",
    "    for traj, status in all_unique_trajs[issue]:\n",
    "        last_edit_idx = {}\n",
    "        reproduce_file_content = {}\n",
    "        main_edit_files = {}\n",
    "        created_files = list(set(get_all_created_files(traj)))\n",
    "        edited_files = list(get_all_edits_files(traj))\n",
    "        for file in created_files:\n",
    "            last_edit_id = -1\n",
    "            for obs, idx in edited_files:\n",
    "                edited_file = obs.split('(Open file: ')[-1].split(')')[0]\n",
    "                if file == edited_file:\n",
    "                    reproduce_file_content[file] = re.sub(pattern, '', obs.split('[File updated')[0])\n",
    "                    last_edit_id = idx\n",
    "            last_edit_idx[file] = last_edit_id\n",
    "        for obs, idx in edited_files:\n",
    "            edited_file = obs.split('(Open file: ')[-1].split(')')[0]\n",
    "            if edited_file not in created_files:\n",
    "                main_edit_files[edited_file] = re.sub(pattern, '', obs.split('[File updated')[0])\n",
    "        all_created_files[issue].append(reproduce_file_content)\n",
    "        last_edit_index[issue].append(last_edit_idx)\n",
    "        main_edited_files[issue].append((main_edit_files, status))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_p = 0\n",
    "unq = set()\n",
    "for issue in main_edited_files:\n",
    "    st = 0\n",
    "    for traj, status in main_edited_files[issue]:\n",
    "        st = st | status\n",
    "    for traj, status in main_edited_files[issue]:\n",
    "        if len(traj) == 0 and st:\n",
    "            r_p += 1\n",
    "            unq.add(issue)\n",
    "r_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Execution Output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_execution_actions(traj):\n",
    "    execution_actions = []\n",
    "    for idx, (act, obs, _) in enumerate(traj):\n",
    "        action = act.split()[0]\n",
    "        if action in ['execute_ipython', 'execute_server', 'python', 'python2', 'python3', 'django-admin', 'pylint', 'pytest', 'sphinx-build', 'tox']:\n",
    "            execution_actions.append((act, obs, idx))\n",
    "    return execution_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_action_outputs = {}\n",
    "for issue in all_unique_trajs:\n",
    "    edit_locations = last_edit_index[issue]\n",
    "    execution_action_outputs[issue] = []\n",
    "    for id, (traj, _) in enumerate(all_unique_trajs[issue]):\n",
    "        edit_location = edit_locations[id]\n",
    "        cur_info = {}\n",
    "        reproduce_files = list(all_created_files[issue][id].keys())\n",
    "        execution_actions = get_execution_actions(traj)\n",
    "        for act, obs, idx in execution_actions:\n",
    "            if act.startswith('execute_ipython'):\n",
    "                cur_info['execute_ipython'] = {'action': act, 'output': obs.split('(Open file: ')[0]}\n",
    "            for f in reproduce_files:\n",
    "                if f.split('/')[-1] in \" \".join(act.split()[1:]):\n",
    "                    edit_idx = edit_location[f]\n",
    "                    if idx <= edit_idx:\n",
    "                        continue\n",
    "                    cur_info[f] = {'action': act, 'output': obs.split('(Open file: ')[0]}\n",
    "                    \n",
    "        execution_action_outputs[issue].append(cur_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relative_path(full_path):\n",
    "    \"\"\"\n",
    "    Convert repository path to relative path.\n",
    "    E.g., /repo_name/path/to/file -> path/to/file\n",
    "    \"\"\"\n",
    "    parts = full_path.split('/', 2)  # Split into [empty, repo_name, rest]\n",
    "    if len(parts) >= 3:\n",
    "        return parts[2]  # Return everything after repo_name\n",
    "    return full_path.lstrip('/')  # Fallback: just remove leading slash\n",
    "\n",
    "def filter_test_changes(patch_content, test_files):\n",
    "    \n",
    "    relative_test_files = {get_relative_path(path) for path in test_files}\n",
    "    sections = patch_content.split('\\ndiff --git ')\n",
    "    \n",
    "    if sections[0].startswith('diff --git '):\n",
    "        sections[0] = sections[0][len('diff --git '):]\n",
    "    \n",
    "    filtered_sections = []\n",
    "    \n",
    "    for section in sections:\n",
    "        if not section.strip():\n",
    "            continue\n",
    "        if filtered_sections:\n",
    "            section = 'diff --git ' + section\n",
    "        else:\n",
    "            section = 'diff --git ' + section\n",
    "            \n",
    "        try:\n",
    "            file_path = section.split(' b/', 1)[1].split('\\n', 1)[0]\n",
    "            file_path = file_path.strip()\n",
    "        \n",
    "            if file_path.startswith('a/'):\n",
    "                file_path = file_path[2:]\n",
    "            \n",
    "            if file_path not in relative_test_files:\n",
    "                filtered_sections.append(section)\n",
    "                \n",
    "        except IndexError:\n",
    "            filtered_sections.append(section)\n",
    "    \n",
    "    result = ''.join(filtered_sections)\n",
    "    \n",
    "    if not result.strip():\n",
    "        return ''\n",
    "        \n",
    "    return result\n",
    "\n",
    "def filter_allowed_changes(patch_content, allowed_files):\n",
    "\n",
    "    relative_allowed_files = {get_relative_path(path) for path in allowed_files}\n",
    "    sections = patch_content.split('\\ndiff --git ')\n",
    "    \n",
    "    if sections[0].startswith('diff --git '):\n",
    "        sections[0] = sections[0][len('diff --git '):]\n",
    "    \n",
    "    filtered_sections = []\n",
    "    \n",
    "    for section in sections:\n",
    "        if not section.strip():\n",
    "            continue\n",
    "        section = 'diff --git ' + section\n",
    "            \n",
    "        try:\n",
    "            file_path = section.split(' b/', 1)[1].split('\\n', 1)[0]\n",
    "            file_path = file_path.strip()\n",
    "            \n",
    "            if file_path.startswith('a/'):\n",
    "                file_path = file_path[2:]\n",
    "            \n",
    "            if file_path in relative_allowed_files:\n",
    "                filtered_sections.append(section)\n",
    "                \n",
    "        except IndexError:\n",
    "            continue\n",
    "    \n",
    "    result = ''.join(filtered_sections)\n",
    "    \n",
    "    if not result.strip():\n",
    "        return ''\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_patches = {}\n",
    "cleaned_patches_to_original = {}\n",
    "for issue in all_unique_trajs:\n",
    "    all_patches[issue] = []\n",
    "    cleaned_patches_to_original[issue] = {}\n",
    "    for id, (traj, status) in enumerate(all_unique_trajs[issue]):\n",
    "        patch =  traj[-1][1].split('(Open file: ')[0]\n",
    "        all_main_files = list(main_edited_files[issue][id][0].keys())\n",
    "        clean_patch = filter_allowed_changes(patch, all_main_files)\n",
    "        all_patches[issue].append((clean_patch, status))\n",
    "        if clean_patch not in cleaned_patches_to_original[issue]:\n",
    "            cleaned_patches_to_original[issue][clean_patch] = [patch]\n",
    "        else:\n",
    "            cleaned_patches_to_original[issue][clean_patch].append(patch)\n",
    "    cleaned_patches_to_original[issue] = {k: v for k, v in sorted(cleaned_patches_to_original[issue].items(), key=lambda item: len(item[1]))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_unique_patches = {}\n",
    "for issue in all_patches:\n",
    "    all_unique_patches[issue] = list(set(all_patches[issue]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 0\n",
    "solved = []\n",
    "for issue in all_unique_patches:\n",
    "    traj_patches = all_unique_patches[issue]\n",
    "    status = 0\n",
    "    l += len(traj_patches)\n",
    "    for _, stat in traj_patches:\n",
    "        status = status | stat\n",
    "    if not status:\n",
    "        continue\n",
    "    solve = 0\n",
    "    total = len(traj_patches)\n",
    "    for patch, stat in traj_patches:\n",
    "        if stat:\n",
    "            solve += 1\n",
    "    solved.append(solve / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajs_context = {}\n",
    "for issue in all_unique_trajs:\n",
    "    trajs_context[issue] = []\n",
    "    created_files = all_created_files[issue]\n",
    "    main_edit_files = main_edited_files[issue]\n",
    "    execution_actions = execution_action_outputs[issue]\n",
    "    patches = all_patches[issue]\n",
    "    for created_file, (main_edit_file, status), execution_action, (patch, _) in zip(created_files, main_edit_files, execution_actions, patches):\n",
    "        trajs_context[issue].append((created_file, main_edit_file, execution_action, patch, status))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Context Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_reproduction_context(created_file):\n",
    "    if not created_file:\n",
    "        return \"No Reproduction Script Created.\"\n",
    "    c_f = \"\\n<reproduction_script>\\n\"\n",
    "    for i in created_file:\n",
    "        c_f += f'File: {i}\\n'\n",
    "        c_f += f'Content:\\n{created_file[i]}\\n\\n'\n",
    "    return c_f + '\\n</reproduction_script>\\n'\n",
    "\n",
    "def handle_bugfix_context(main_edit_file):\n",
    "    if not main_edit_file:\n",
    "        return \"No edits are made to fix the issue.\"\n",
    "    c_f = \"\\n<updated_file>\\n\"\n",
    "    for i in main_edit_file:\n",
    "        c_f += f'File: {i}\\n'\n",
    "        c_f += f'Updated Content:\\n{main_edit_file[i]}\\n\\n'\n",
    "    return c_f + '\\n</updated_file>\\n'\n",
    "\n",
    "def handle_execution_context(execution_action):\n",
    "    if not execution_action:\n",
    "        return \"Final edits are not tested before submission.\"\n",
    "    c_f = \"\\n<execution_output>\\n\"\n",
    "    for i in execution_action:\n",
    "        if i == 'execute_ipython':\n",
    "            c_f += f'Following code executed in jupyter notebook: {\" \".join(execution_action[i][\"action\"].split()[1:])}\\n'\n",
    "            c_f += f'Output:\\n{execution_action[i][\"output\"]}\\n\\n'\n",
    "        else:\n",
    "            c_f += f'Execution Command: {execution_action[i][\"action\"]}\\n'\n",
    "            c_f += f'Output:\\n{execution_action[i][\"output\"]}\\n\\n'\n",
    "    return c_f + '\\n</execution_output>\\n'\n",
    "\n",
    "def handle_patch_context(patch):\n",
    "    if not patch:\n",
    "        return \"No patch is submitted.\"\n",
    "    while patch[-1] == '\\n':\n",
    "        patch = patch[:-1]\n",
    "    return f'\\n<patch>\\n{patch}\\n<\\patch>\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajs_final_context = {}\n",
    "for issue in trajs_context:\n",
    "    trajs_final_context[issue] = []\n",
    "    for created_file, main_edit_file, execution_action, patch, status in trajs_context[issue]:\n",
    "        context = \"\"\n",
    "        # context += handle_reproduction_context(created_file) # uncomment if adding reproduction script while evaluation\n",
    "        # context += handle_bugfix_context(main_edit_file) # uncomment if adding edited script while evaluation\n",
    "        # context += handle_execution_context(execution_action) # uncomment if adding execution output while evaluation\n",
    "        context += handle_patch_context(patch)\n",
    "        trajs_final_context[issue].append((context, status))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_components(context):\n",
    "    \"\"\"Count the number of non-empty components in a trajectory.\"\"\"\n",
    "    components = {\n",
    "        \"reproduction\": \"No Reproduction Script Created.\" not in context,\n",
    "        \"bugfix\": \"No edits are made to fix the issue.\" not in context,\n",
    "        \"execution\": \"Final edits are not tested before submission.\" not in context,\n",
    "        \"patch\": \"No patch is submitted.\" not in context\n",
    "    }\n",
    "    return sum(components.values()), components\n",
    "\n",
    "def get_component_score(components):\n",
    "    \"\"\"Calculate score based on component preferences.\"\"\"\n",
    "    score = 0\n",
    "    if components[\"bugfix\"]:\n",
    "        score += 4\n",
    "    if components[\"reproduction\"]:\n",
    "        score += 2\n",
    "    if components[\"execution\"]:\n",
    "        score += 1\n",
    "    return score\n",
    "\n",
    "def filter_trajectories(trajs_final_context):\n",
    "    filtered_trajs_final_context = {}\n",
    "    for issue in trajs_final_context:\n",
    "        # Group trajectories by patch\n",
    "        patch_groups = {}\n",
    "        for context, status in trajs_final_context[issue]:\n",
    "            # Skip if no patch\n",
    "            if \"No patch is submitted.\" in context:\n",
    "                continue\n",
    "                \n",
    "            # Extract patch from context\n",
    "            patch_start = context.find('\\n<patch>\\n')\n",
    "            if patch_start == -1:\n",
    "                continue\n",
    "            patch = context[patch_start:]\n",
    "            \n",
    "            if patch not in patch_groups:\n",
    "                patch_groups[patch] = []\n",
    "            patch_groups[patch].append((context, status))\n",
    "\n",
    "        # Process each patch group\n",
    "        best_trajectories = []\n",
    "        for patch, trajectories in patch_groups.items():\n",
    "            # Find trajectory with best components\n",
    "            max_components = 0\n",
    "            max_component_score = 0\n",
    "            min_context_length = float('inf')\n",
    "            best_trajectory = None\n",
    "            \n",
    "            for context, status in trajectories:\n",
    "                num_components, components = count_components(context)\n",
    "                component_score = get_component_score(components)\n",
    "                context_length = len(context)\n",
    "                \n",
    "                # Update best trajectory based on criteria\n",
    "                if num_components > max_components:\n",
    "                    max_components = num_components\n",
    "                    max_component_score = component_score\n",
    "                    min_context_length = context_length\n",
    "                    best_trajectory = (context, status)\n",
    "                elif num_components == max_components:\n",
    "                    if component_score > max_component_score:\n",
    "                        max_component_score = component_score\n",
    "                        min_context_length = context_length\n",
    "                        best_trajectory = (context, status)\n",
    "                    elif component_score == max_component_score:\n",
    "                        if context_length < min_context_length:\n",
    "                            min_context_length = context_length\n",
    "                            best_trajectory = (context, status)\n",
    "            \n",
    "            if best_trajectory:\n",
    "                best_trajectories.append(best_trajectory)\n",
    "        \n",
    "        filtered_trajs_final_context[issue] = best_trajectories\n",
    "    \n",
    "    return filtered_trajs_final_context\n",
    "\n",
    "filtered_trajs_final_context = filter_trajectories(trajs_final_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solved = []\n",
    "solved_issues = set()\n",
    "solved_ratio = []\n",
    "for issue in filtered_trajs_final_context:\n",
    "    solve = 0\n",
    "    total = len(filtered_trajs_final_context[issue])\n",
    "    for context, status in filtered_trajs_final_context[issue]:\n",
    "        if status:\n",
    "            solve += 1\n",
    "    if solve != 0:\n",
    "        solved.append((solve, total))\n",
    "        solved_ratio.append(solve / total)\n",
    "        solved_issues.add(issue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(solved), round(sum(solved_ratio) / len(solved_ratio), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_issues = sorted(solved_issues, key=lambda x: sum([1 for context, status in filtered_trajs_final_context[x] if status]) / len(filtered_trajs_final_context[x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = '''You are an expert software engineering evaluator tasked with analyzing and selecting the best solution patch for a GitHub issue. Your goal is to thoroughly evaluate each proposed patch and determine which one most effectively solves the issue.\n",
    "\n",
    "First, examine the proposed patches:\n",
    "\n",
    "<patches>\n",
    "{trajs}\n",
    "</patches>\n",
    "\n",
    "Now, carefully read the following GitHub issue:\n",
    "\n",
    "<github_issue>\n",
    "{issue}\n",
    "</github_issue>\n",
    "\n",
    "Your task is to evaluate each patch based on its effectiveness in fixing the bug. Use the following scoring system:\n",
    "\n",
    "1. Bug Fixing Score (0-2):\n",
    "   0: Incorrect changes that won't fix the issue\n",
    "   1: Partially correct changes (might fix the issue but misses corner cases)\n",
    "   2: Correct changes that will fully fix the issue\n",
    "\n",
    "2. Regression Risk (Score 0-2):\n",
    "   0: High risk of introducing multiple regression bugs\n",
    "   1: Moderate risk of introducing a few regression bugs\n",
    "   2: Low risk of introducing any regression bugs\n",
    "\n",
    "For each patch, provide your analysis using the following structure:\n",
    "\n",
    "<patch_analysis>\n",
    "  <patch_number>[Patch number]</patch_number>\n",
    "  <bug_fixing_analysis>\n",
    "    [Your analysis of the bug fixing approach]\n",
    "    <score>[Score (0-2)]</score>\n",
    "  </bug_fixing_analysis>\n",
    "  <regression_risk_analysis>\n",
    "    [Your analysis of potential regression risks]\n",
    "    <score>[Score (0-2)]</score>\n",
    "  </regression_risk_analysis>\n",
    "</patch_analysis>\n",
    "\n",
    "In your analysis, consider:\n",
    "1. How well the patch addresses the core issue described in the GitHub issue?\n",
    "2. Will the patch introduce regression bugs or other issues?\n",
    "\n",
    "Please reason step by step, and put your final answer in the following format:\n",
    "\n",
    "<best_patch>Number of the best patch</best_patch>\n",
    "\n",
    "Important Tips:\n",
    "1. For each patch, write down the main changes and their potential impact\n",
    "2. Compare patches side-by-side\n",
    "3. Consider potential edge cases and how each patch addresses them\n",
    "4. Please do not prioritize one patch over another based on the position in the list. Evaluate each patch independently.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_dataset(\"princeton-nlp/SWE-bench_Lite\", split=\"test\")\n",
    "df = pd.DataFrame(data)\n",
    "dct = {}\n",
    "for i, row in df.iterrows():\n",
    "    dct[row['instance_id']] = row['problem_statement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_best_trajectory(text):\n",
    "    # Look for the best trajectory tag\n",
    "    start_tag = \"<best_patch>\"\n",
    "    end_tag = \"</best_patch>\"\n",
    "    \n",
    "    try:\n",
    "        # Find the start and end positions of the content\n",
    "        start_pos = text.find(start_tag) + len(start_tag)\n",
    "        end_pos = text.find(end_tag)\n",
    "        \n",
    "        # Extract and return the content between tags\n",
    "        if start_pos >= 0 and end_pos >= 0:\n",
    "            return text[start_pos:end_pos].strip()\n",
    "        else:\n",
    "            return text\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing best trajectory: {e}\")\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_issues = sorted_issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger('httpx').setLevel(logging.ERROR)\n",
    "logging.getLogger('LiteLLM').setLevel(logging.ERROR)\n",
    "\n",
    "nest_asyncio.apply()\n",
    "thread_pool = ThreadPoolExecutor(max_workers=16)\n",
    "\n",
    "async def process_single_issue(issue, sem, filtered_trajs_final_context, dct, prompt_template, completion):\n",
    "    async with sem:\n",
    "        context = \"\\n\".join([f\"Solution Patch {idx + 1}: {i[0]}\" \n",
    "                            for idx, i in enumerate(filtered_trajs_final_context[issue])])\n",
    "        prompt = prompt_template.format(issue=dct[issue], trajs=context)\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "        model_params = {\n",
    "            \"model\": \"gpt-4o\",\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": 0.6,\n",
    "            \"top_p\": 0.95,\n",
    "        }\n",
    "        try:\n",
    "            # Run the synchronous completion in the thread pool\n",
    "            loop = asyncio.get_event_loop()\n",
    "            response = await loop.run_in_executor(\n",
    "                thread_pool,\n",
    "                lambda: completion(**model_params)\n",
    "            )\n",
    "            content = response.choices[0].message.content\n",
    "            # best_trajectory = parse_best_trajectory(content)\n",
    "            best_trajectory = content\n",
    "            return issue, best_trajectory\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing issue {issue}: {str(e)}\")\n",
    "            return issue, None\n",
    "\n",
    "async def run_parallel_processing():\n",
    "    sem = Semaphore(16)  # Limit to 16 concurrent tasks\n",
    "    tasks = []\n",
    "    best_trajs = {}\n",
    "    \n",
    "    # Create all tasks\n",
    "    for issue in target_issues:\n",
    "        task = process_single_issue(\n",
    "            issue, sem, filtered_trajs_final_context, dct, \n",
    "            prompt_template, completion\n",
    "        )\n",
    "        tasks.append(task)\n",
    "    \n",
    "    # Process tasks with progress bar\n",
    "    for future in tqdm(asyncio.as_completed(tasks), total=len(tasks)):\n",
    "        issue, best_trajectory = await future\n",
    "        if best_trajectory is not None:\n",
    "            best_trajs[issue] = best_trajectory\n",
    "    \n",
    "    return best_trajs\n",
    "\n",
    "best_trajs = await run_parallel_processing()\n",
    "thread_pool.shutdown(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rem = []\n",
    "correct = set()\n",
    "for issue in target_issues:\n",
    "    try:\n",
    "        pred = int(parse_best_trajectory(best_trajs[issue]))\n",
    "        if filtered_trajs_final_context[issue][pred - 1][1]:\n",
    "            correct.add(issue)\n",
    "    except:\n",
    "        print(issue)\n",
    "        rem.append(issue)\n",
    "len(correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()\n",
    "thread_pool = ThreadPoolExecutor(max_workers=16)\n",
    "\n",
    "async def process_single_issue(issue, sem, eval_out, completion):\n",
    "    async with sem:\n",
    "        prompt = \"This is the evaluation output for the patches but this is not parse properly. Here is the output:\\n<evaluation_result>\\n{evaluation_result}\\n</evaluation_result>\\n I want you to give me parse output so that I can evaluate the patches properly. \\n For that please just give me the best patch number using the following output schema <best_patch>Number of the best patch</best_patch>.\".format(evaluation_result=eval_out)\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "        model_params = {\n",
    "            \"model\": \"gpt-4o\",\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": 0.0,\n",
    "            \"top_p\": 0.95\n",
    "        }\n",
    "        try:\n",
    "            loop = asyncio.get_event_loop()\n",
    "            response = await loop.run_in_executor(\n",
    "                thread_pool,\n",
    "                lambda: completion(**model_params)\n",
    "            )\n",
    "            content = response.choices[0].message.content\n",
    "            best_trajectory = parse_best_trajectory(content)\n",
    "            return issue, best_trajectory\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing issue {issue}: {str(e)}\")\n",
    "            return issue, None\n",
    "\n",
    "async def run_parallel_processing():\n",
    "    sem = Semaphore(8)  # Limit to 16 concurrent tasks\n",
    "    tasks = []\n",
    "    final_best_trajs = {}\n",
    "    \n",
    "    # Create all tasks\n",
    "    for issue in rem:\n",
    "        best_traj = best_trajs[issue] if issue in best_trajs else \"\"\n",
    "        task = process_single_issue(\n",
    "            issue, sem, best_traj, completion\n",
    "        )\n",
    "        tasks.append(task)\n",
    "    \n",
    "    # Process tasks with progress bar\n",
    "    for future in tqdm(asyncio.as_completed(tasks), total=len(tasks)):\n",
    "        issue, best_trajectory = await future\n",
    "        if best_trajectory is not None:\n",
    "            final_best_trajs[issue] = best_trajectory\n",
    "    \n",
    "    return final_best_trajs\n",
    "\n",
    "final_best_trajs = await run_parallel_processing()\n",
    "thread_pool.shutdown(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for issue in rem:\n",
    "    try:\n",
    "        pred = int(final_best_trajs[issue])\n",
    "        if filtered_trajs_final_context[issue][pred - 1][1]:\n",
    "            correct.add(issue)\n",
    "    except:\n",
    "        print(issue)\n",
    "len(correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pair-wise Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()\n",
    "thread_pool = ThreadPoolExecutor(max_workers=16)\n",
    "\n",
    "async def compare_pair(traj1_idx, traj2_idx, issue, sem, filtered_trajs_final_context, dct, prompt_template, completion, round_num, comparison_results):\n",
    "    \"\"\"Compare two trajectories and return the index of the better one along with the comparison details.\"\"\"\n",
    "    async with sem:\n",
    "        # Format the two trajectories for comparison\n",
    "        traj1 = filtered_trajs_final_context[issue][traj1_idx - 1][0]\n",
    "        traj2 = filtered_trajs_final_context[issue][traj2_idx - 1][0]\n",
    "        \n",
    "        context = f\"Solution Description 1: {traj1}\\nSolution Description 2: {traj2}\"\n",
    "        prompt = prompt_template.format(issue=dct[issue], trajs=context)\n",
    "\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        model_params = {\n",
    "            \"model\": \"gpt-4o\",\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": 0.0,\n",
    "            \"top_p\": 0.95,\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            loop = asyncio.get_event_loop()\n",
    "            response = await loop.run_in_executor(\n",
    "                thread_pool,\n",
    "                lambda: completion(**model_params)\n",
    "            )\n",
    "            content = response.choices[0].message.content\n",
    "            best_trajectory = parse_best_trajectory(content)\n",
    "            winner = traj1_idx if best_trajectory == 1 else traj2_idx\n",
    "            \n",
    "            # Store comparison results\n",
    "            comparison_info = {\n",
    "                \"round\": round_num,\n",
    "                \"traj1_idx\": traj1_idx,\n",
    "                \"traj2_idx\": traj2_idx,\n",
    "                \"model_output\": content,\n",
    "                \"winner\": winner\n",
    "            }\n",
    "            comparison_results.append(comparison_info)\n",
    "            \n",
    "            return winner\n",
    "        except Exception as e:\n",
    "            print(f\"Error comparing trajectories {traj1_idx} and {traj2_idx} for issue {issue}: {str(e)}\")\n",
    "            # Store error information\n",
    "            comparison_info = {\n",
    "                \"round\": round_num,\n",
    "                \"traj1_idx\": traj1_idx,\n",
    "                \"traj2_idx\": traj2_idx,\n",
    "                \"error\": str(e),\n",
    "                \"winner\": traj1_idx  # Default to first trajectory in case of error\n",
    "            }\n",
    "            comparison_results.append(comparison_info)\n",
    "            return traj1_idx\n",
    "\n",
    "async def process_single_issue_tournament(issue, sem, filtered_trajs_final_context, base_save_dir, dct, prompt_template, completion):\n",
    "    \"\"\"Process a single issue using tournament-style elimination and save comparison results.\"\"\"\n",
    "    num_trajs = len(filtered_trajs_final_context[issue])\n",
    "    current_round = list(range(1, num_trajs + 1))\n",
    "    round_num = 1\n",
    "    \n",
    "    save_dir = f\"{base_save_dir}/{issue}\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    all_rounds_results = []\n",
    "    \n",
    "    while len(current_round) > 1:\n",
    "        print(f\"Issue {issue} - Round {round_num} - {len(current_round)} trajectories remaining\")\n",
    "        next_round = []\n",
    "        round_results = []\n",
    "        \n",
    "        # Create pairs for this round\n",
    "        for i in range(0, len(current_round), 2):\n",
    "            if i + 1 >= len(current_round):\n",
    "                next_round.append(current_round[i])\n",
    "                continue\n",
    "                \n",
    "            # Compare pair and get winner\n",
    "            winner = await compare_pair(\n",
    "                current_round[i], \n",
    "                current_round[i + 1],\n",
    "                issue,\n",
    "                sem,\n",
    "                filtered_trajs_final_context,\n",
    "                dct,\n",
    "                prompt_template,\n",
    "                completion,\n",
    "                round_num,\n",
    "                round_results\n",
    "            )\n",
    "            next_round.append(winner)\n",
    "        \n",
    "        # Save results for this round\n",
    "        round_filename = os.path.join(save_dir, f\"round_{round_num}_results.json\")\n",
    "        with open(round_filename, 'w') as f:\n",
    "            json.dump({\n",
    "                \"round_number\": round_num,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"comparisons\": round_results,\n",
    "                \"advancing_trajectories\": next_round\n",
    "            }, f, indent=2)\n",
    "        \n",
    "        all_rounds_results.extend(round_results)\n",
    "        current_round = next_round\n",
    "        round_num += 1\n",
    "    \n",
    "    # Save complete tournament results\n",
    "    final_filename = os.path.join(save_dir, \"complete_tournament_results.json\")\n",
    "    with open(final_filename, 'w') as f:\n",
    "        json.dump({\n",
    "            \"issue\": issue,\n",
    "            \"total_rounds\": round_num - 1,\n",
    "            \"winner\": current_round[0],\n",
    "            \"all_comparisons\": all_rounds_results\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    return issue, current_round[0]\n",
    "\n",
    "async def run_parallel_tournament(base_save_dir):\n",
    "    sem = Semaphore(16)  # Limit to 16 concurrent tasks\n",
    "    tasks = []\n",
    "    best_trajs = {}\n",
    "    \n",
    "    # Create tasks for each issue\n",
    "    for issue in target_issues:\n",
    "        task = process_single_issue_tournament(\n",
    "            issue, sem, filtered_trajs_final_context, base_save_dir, dct, prompt_template, completion\n",
    "        )\n",
    "        tasks.append(task)\n",
    "    \n",
    "    # Process issues in parallel with progress bar\n",
    "    for future in tqdm(asyncio.as_completed(tasks), total=len(tasks)):\n",
    "        issue, best_trajectory_idx = await future\n",
    "        best_trajs[issue] = best_trajectory_idx\n",
    "    \n",
    "    return best_trajs\n",
    "\n",
    "# Execute the tournament processing\n",
    "best_trajs = await run_parallel_tournament(base_save_dir='/path/to/save_results/')\n",
    "\n",
    "# Clean up the thread pool when done\n",
    "thread_pool.shutdown(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = set()\n",
    "for issue in target_issues:\n",
    "    try:\n",
    "        pred = best_trajs[issue]\n",
    "        if filtered_trajs_final_context[issue][pred - 1][1]:\n",
    "            correct.add(issue)\n",
    "    except:\n",
    "        print(issue)\n",
    "len(correct)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swe-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
