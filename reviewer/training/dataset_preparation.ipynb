{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from litellm import encode\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "from datasets import load_dataset\n",
    "from pandarallel import pandarallel\n",
    "from swebench.harness.constants import TestStatus\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "pandarallel.initialize(progress_bar=False, nb_workers=8)\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_status(content):\n",
    "    if 'Some tests failed.' in content:\n",
    "        return 0\n",
    "    if 'All tests passed.' in content:\n",
    "        return 1\n",
    "    return -1\n",
    "\n",
    "def get_token_count(text):\n",
    "    return len(encode(model=\"gpt-4o\", text=text))\n",
    "\n",
    "def extract_code_blocks(text: str) -> list[str]:\n",
    "   pattern = r'```(?:\\w+)?\\n(.*?)```'\n",
    "   matches = re.findall(pattern, text, re.DOTALL) \n",
    "   return matches\n",
    "\n",
    "def get_create_files(traj):\n",
    "    create_files = []\n",
    "    for idx, event in enumerate(traj):\n",
    "        if event[\"text\"] is None:\n",
    "            continue\n",
    "        action = extract_code_blocks(event[\"text\"])\n",
    "        if len(action) == 0:\n",
    "            continue\n",
    "        action = action[0]\n",
    "        if action.split(\" \")[0] == \"create\":\n",
    "            if idx + 1 >= len(traj):\n",
    "                continue\n",
    "            create_file = traj[idx + 1]['text'].split('(Open file: ')[-1].split(')')[0]\n",
    "            create_files.append(create_file)\n",
    "    return create_files\n",
    "\n",
    "def get_modified_files(traj):\n",
    "    modified_files = []\n",
    "    for idx, event in enumerate(traj):\n",
    "        if event[\"text\"] is None:\n",
    "            continue\n",
    "        action = extract_code_blocks(event[\"text\"])\n",
    "        if len(action) == 0:\n",
    "            continue\n",
    "        action = action[0]\n",
    "        if action.split()[0] == \"sed\":\n",
    "            modified_file = action.split(\" \")[-1].split('\\n')[0]\n",
    "            modified_files.append(modified_file)\n",
    "        elif action.split()[0] == \"edit\":\n",
    "            if idx + 1 >= len(traj):\n",
    "                continue\n",
    "            modified_file = traj[idx + 1]['text'].split('(Open file: ')[-1].split(')')[0]\n",
    "            modified_files.append(modified_file)\n",
    "        elif action.split()[0] == \"echo\":\n",
    "            if \">>\" not in action:\n",
    "                continue\n",
    "            modified_file = action.split(\" \")[-1].split('\\n')[0]\n",
    "            modified_files.append(modified_file)\n",
    "    return modified_files\n",
    "\n",
    "def get_files_from_patch(patch_content: str) -> list[str]:\n",
    "    files = []\n",
    "    lines = patch_content.split('\\n')\n",
    "    \n",
    "    for line in lines:\n",
    "        if line.startswith('diff --git'):\n",
    "            # Extract both a/ and b/ paths\n",
    "            parts = line.split()\n",
    "            a_file = parts[2][2:]  # Remove a/ prefix\n",
    "            b_file = parts[3][2:]  # Remove b/ prefix\n",
    "            if a_file not in files:\n",
    "                files.append(a_file)\n",
    "            if b_file != a_file and b_file not in files:\n",
    "                files.append(b_file)\n",
    "    return files\n",
    "\n",
    "def get_new_files_from_patch(patch_content: str) -> list[str]:\n",
    "    \"\"\"Get list of newly created files from git patch.\"\"\"\n",
    "    new_files = []\n",
    "    lines = patch_content.splitlines()\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.startswith('new file mode'):\n",
    "            # Get the preceding diff line which contains the file path\n",
    "            diff_line = lines[i-1]\n",
    "            file_path = diff_line.split()[-1][2:]  # Remove b/ prefix\n",
    "            new_files.append(file_path)\n",
    "    return new_files\n",
    "\n",
    "def get_relative_path(full_path):\n",
    "    parts = full_path.split('/', 2)  # Split into [empty, repo_name, rest]\n",
    "    if len(parts) >= 3:\n",
    "        return parts[2]  # Return everything after repo_name\n",
    "    return full_path.lstrip('/')  # Fallback: just remove leading slash\n",
    "\n",
    "def filter_allowed_changes(patch_content, allowed_files):\n",
    "    relative_allowed_files = {get_relative_path(path) for path in allowed_files}\n",
    "    sections = patch_content.split('\\ndiff --git ')\n",
    "    \n",
    "    if sections[0].startswith('diff --git '):\n",
    "        sections[0] = sections[0][len('diff --git '):]\n",
    "    \n",
    "    filtered_sections = []\n",
    "    \n",
    "    for section in sections:\n",
    "        if not section.strip():\n",
    "            continue\n",
    "        section = 'diff --git ' + section\n",
    "            \n",
    "        try:\n",
    "            file_path = section.split(' b/', 1)[1].split('\\n', 1)[0]\n",
    "            file_path = file_path.strip()\n",
    "            \n",
    "            if file_path.startswith('a/'):\n",
    "                file_path = file_path[2:]\n",
    "            \n",
    "            if file_path in relative_allowed_files:\n",
    "                filtered_sections.append(section)\n",
    "                \n",
    "        except IndexError:\n",
    "            continue\n",
    "    \n",
    "    result = ''.join(filtered_sections)\n",
    "    \n",
    "    if not result.strip():\n",
    "        return ''\n",
    "        \n",
    "    return result\n",
    "\n",
    "def get_deleted_files(patch_content):\n",
    "    deleted_files = []\n",
    "    current_file = None\n",
    "    is_deleted = False\n",
    "    for line in patch_content.splitlines():\n",
    "        # Check for diff headers\n",
    "        if line.startswith('diff --git'):\n",
    "            current_file = None\n",
    "            is_deleted = False\n",
    "        # Check for file deletions\n",
    "        elif line.startswith('deleted file mode'):\n",
    "            is_deleted = True\n",
    "        # Get the file path from --- line\n",
    "        elif line.startswith('--- a/'):\n",
    "            current_file = line[6:]  # Remove '--- a/' prefix\n",
    "            if is_deleted and current_file:\n",
    "                deleted_files.append(current_file)\n",
    "    return deleted_files\n",
    "\n",
    "def if_remove_file(file_path: str, deleted_files: set[str], created_files: set[str]) -> bool:\n",
    "    EXCLUDE_PATTERNS = [\n",
    "        r'reproduc', r'debug', r'fix', r'test',\n",
    "        r'\\.pyc$', r'\\.cow$', r'\\.zip$', r'\\.whl$', r'\\.dvc$', \n",
    "        r'\\.deb$', r'\\.dcm$', r'\\.gz$', r'\\.tar$',\n",
    "        r'\\.txt$', r'\\.md$', r'\\.rst$',\n",
    "        r'\\.\\d+', r'gitignore',\n",
    "        r'venv/bin/', r'eggs', r'dist', r'egg-info'\n",
    "    ]\n",
    "    if \"htm\" in file_path and (file_path in deleted_files or file_path in created_files):\n",
    "        return True\n",
    "    return any(re.search(pattern, file_path) for pattern in EXCLUDE_PATTERNS)\n",
    "\n",
    "def clean_patch(patch, modified_files, created_files):\n",
    "    all_files = get_files_from_patch(patch)\n",
    "    deleted_files = set(get_deleted_files(patch))\n",
    "    to_remove_files = [i for i in all_files if if_remove_file(i, deleted_files, created_files)]\n",
    "    retain_files = list(set(modified_files) - set(created_files) - set(to_remove_files) - set(deleted_files))\n",
    "    return filter_allowed_changes(patch, retain_files)\n",
    "\n",
    "def parse_log_pytest(log: str) -> dict[str, str]:\n",
    "    test_status_map = {}\n",
    "    for line in log.split(\"\\n\"):\n",
    "        if any([line.startswith(x.value) for x in TestStatus]):\n",
    "            # Additional parsing for FAILED status\n",
    "            if line.startswith(TestStatus.FAILED.value):\n",
    "                line = line.replace(\" - \", \" \")\n",
    "            test_case = line.split()\n",
    "            if len(test_case) <= 1:\n",
    "                continue\n",
    "            test_status_map[test_case[1]] = test_case[0]\n",
    "    return test_status_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe creation and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset('nebius/SWE-agent-trajectories', split='train')\n",
    "df = pd.DataFrame(data)\n",
    "df.fillna('', inplace=True)\n",
    "df['status_msg'] = df['eval_logs'].apply(parse_status)\n",
    "df = df[df['status_msg'] != -1].reset_index(drop=True)\n",
    "df['created_file'] = df['trajectory'].progress_apply(get_create_files)\n",
    "df['modified_file'] = df['trajectory'].progress_apply(get_modified_files)\n",
    "df['generated_patch'] = df.parallel_apply(lambda x: clean_patch(x['generated_patch'], x['modified_file'], x['created_file']), axis=1)\n",
    "df.drop_duplicates(subset=['generated_patch'], inplace=True, ignore_index=True)\n",
    "df = df[df['generated_patch'] != ''].reset_index(drop=True)\n",
    "df['token_count'] = df['generated_patch'].parallel_apply(get_token_count)\n",
    "df['eval_token_count'] = df['eval_logs'].parallel_apply(get_token_count)\n",
    "df = df[df['token_count'] <= 4000].reset_index(drop=True)\n",
    "df['test_status'] = df['eval_logs'].parallel_apply(parse_log_pytest)\n",
    "df = df[df['test_status'] != {}].reset_index(drop=True)\n",
    "df.to_csv('reward_data/initial_reward_model_train.csv', index=False)\n",
    "df = df[['instance_id', 'generated_patch', 'test_status', 'target', 'token_count']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(x):\n",
    "    try:\n",
    "        return eval(x)\n",
    "    except:\n",
    "        return x\n",
    "\n",
    "def get_tests(test_status, tests):\n",
    "    return [test for test in tests if test_status[test] == 'FAILED']\n",
    "\n",
    "def get_bugfixing_status(f2p_failed, f2p):\n",
    "    if len(f2p_failed) == 0:\n",
    "        return 2\n",
    "    if len(f2p) == len(f2p_failed):\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "def new_issues_status(p2p_failed, p2p):\n",
    "    if len(p2p_failed) == 0:\n",
    "        return 2\n",
    "    if len(p2p) == len(p2p_failed):\n",
    "        return 0\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset('nebius/SWE-bench-extra', split='train')\n",
    "df_bench = pd.DataFrame(data)[['instance_id', 'patch', 'problem_statement', 'FAIL_TO_PASS', 'PASS_TO_PASS']]\n",
    "data_swe = load_dataset('princeton-nlp/SWE-bench', split='dev')\n",
    "df_swe = pd.DataFrame(data_swe)[['instance_id', 'patch', 'problem_statement', 'FAIL_TO_PASS', 'PASS_TO_PASS']]\n",
    "df_bench = pd.concat([df_bench, df_swe], ignore_index=True)\n",
    "df = df.merge(df_bench, on='instance_id', how='left')\n",
    "df['FAIL_TO_PASS'] = df['FAIL_TO_PASS'].progress_apply(evaluate)\n",
    "df['PASS_TO_PASS'] = df['PASS_TO_PASS'].progress_apply(evaluate)\n",
    "df.fillna('', inplace=True)\n",
    "df['P2P_failed'] = df.progress_apply(lambda x: get_tests(x['test_status'], x['PASS_TO_PASS']), axis=1)\n",
    "df['F2P_failed'] = df.progress_apply(lambda x: get_tests(x['test_status'], x['FAIL_TO_PASS']), axis=1)\n",
    "df['bug_fixing'] = df.apply(lambda x: get_bugfixing_status(x['F2P_failed'], x['FAIL_TO_PASS']), axis=1)\n",
    "df['new_issues'] = df.apply(lambda x: new_issues_status(x['P2P_failed'], x['PASS_TO_PASS']), axis=1)\n",
    "df.rename(columns={'P2P_failed': 'p2p_failed', 'F2P_failed': 'f2p_failed'}, inplace=True)\n",
    "df.to_csv('reward_data/final_reward_model_train_before_aug.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Critique Generation\n",
    "\n",
    "After cleaning the patches, the next step is to generate critiques for each patch.  \n",
    "For reference, consult the `readme.md` file located at:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('reward_data/after_critique_generation_gt.csv')\n",
    "df.fillna('', inplace=True)\n",
    "df['test_status'] = df['test_status'].progress_apply(evaluate)\n",
    "df['p2p_failed'] = df['p2p_failed'].progress_apply(evaluate)\n",
    "df['f2p_failed'] = df['f2p_failed'].progress_apply(evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~((df['generated_patch'] == df['patch']) & (df['target'] == 0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from collections import defaultdict \n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import random\n",
    "\n",
    "def get_patch_info(row, if_gt=False):\n",
    "    return {\n",
    "        'patch': row['generated_patch'] if not if_gt else row['patch'],\n",
    "        'target': row['target'] if not if_gt else True,\n",
    "        'p2p_failed': row['p2p_failed'] if not if_gt else [],\n",
    "        'f2p_failed': row['f2p_failed'] if not if_gt else [],\n",
    "        'bug_fixing': row['bug_fixing'] if not if_gt else 2,\n",
    "        'new_issues': row['new_issues'] if not if_gt else 2,\n",
    "        'critique': row['critique'] if not if_gt else row['critique_gt'],\n",
    "    }\n",
    "\n",
    "def sample_incorrect_patches(failed_patches_dict, num_patches_needed, max_samples=3):\n",
    "    total_patches = sum(len(patches) for patches in failed_patches_dict.values())\n",
    "    if total_patches < num_patches_needed:\n",
    "        all_patches = [p for patches in failed_patches_dict.values() for p in patches]\n",
    "        return [random.sample(all_patches, min(num_patches_needed, len(all_patches))) \n",
    "                for _ in range(min(max_samples, len(all_patches)))]\n",
    "    \n",
    "    sampled_combinations = []\n",
    "    for _ in range(max_samples):\n",
    "        selected_patches = []\n",
    "        buckets = list(failed_patches_dict.keys())\n",
    "        patches_needed = num_patches_needed\n",
    "        \n",
    "        while patches_needed > 0 and buckets:\n",
    "            total_remaining = sum(len(failed_patches_dict[k]) for k in buckets)\n",
    "            allocations = []\n",
    "            \n",
    "            for bucket in buckets[:]:\n",
    "                patches = failed_patches_dict[bucket]\n",
    "                proportion = len(patches) / total_remaining\n",
    "                num_from_bucket = min(\n",
    "                    max(1, round(patches_needed * proportion)),\n",
    "                    len(patches),\n",
    "                    patches_needed\n",
    "                )\n",
    "                if num_from_bucket > 0:\n",
    "                    allocations.append((bucket, num_from_bucket))\n",
    "            \n",
    "            random.shuffle(allocations)\n",
    "            for bucket, num_to_select in allocations:\n",
    "                selected = random.sample(failed_patches_dict[bucket], num_to_select)\n",
    "                selected_patches.extend(selected)\n",
    "                patches_needed -= num_to_select\n",
    "                buckets.remove(bucket)\n",
    "                \n",
    "                if patches_needed == 0:\n",
    "                    break\n",
    "        \n",
    "        if selected_patches:\n",
    "            sampled_combinations.append(selected_patches)\n",
    "    \n",
    "    return sampled_combinations\n",
    "\n",
    "def process_instance(group_data):\n",
    "    instance_id, df_inst = group_data\n",
    "    train_rows = []\n",
    "    max_patches = 6\n",
    "    \n",
    "    ground_truth_patch = df_inst.iloc[0]\n",
    "    gt_patch_info = get_patch_info(ground_truth_patch, if_gt=True)\n",
    "    correct_patches = [get_patch_info(row) for _, row in df_inst[df_inst['target'] == 1].iterrows()]\n",
    "    \n",
    "    failed_patches_dict = defaultdict(list)\n",
    "    for _, row in df_inst[df_inst['target'] == 0].iterrows():\n",
    "        failed_tests = tuple(sorted(set(row['p2p_failed'] + row['f2p_failed'])))\n",
    "        failed_patches_dict[failed_tests].append(get_patch_info(row))\n",
    "    \n",
    "    total_solved = len(correct_patches)\n",
    "    \n",
    "    for num_patches in range(2, max_patches + 1):\n",
    "        possible_correct = list(set([0, min(1, total_solved)]))\n",
    "        for n_correct in possible_correct:\n",
    "            n_incorrect = num_patches - n_correct - 1  # -1 for ground truth\n",
    "            \n",
    "            if n_incorrect <= 0:\n",
    "                continue\n",
    "                \n",
    "            if n_correct == 0:\n",
    "                correct_combinations = [()]\n",
    "            else:\n",
    "                correct_combinations = list(combinations(correct_patches, n_correct))\n",
    "            \n",
    "            for correct_combo in correct_combinations:\n",
    "                incorrect_patch_samples = sample_incorrect_patches(\n",
    "                    failed_patches_dict,\n",
    "                    n_incorrect,\n",
    "                    max_samples=2\n",
    "                )\n",
    "                \n",
    "                for incorrect_patches in incorrect_patch_samples:\n",
    "                    if len(incorrect_patches) != n_incorrect:\n",
    "                        continue\n",
    "                        \n",
    "                    row = {\n",
    "                        'instance_id': instance_id,\n",
    "                        'problem_statement': ground_truth_patch['problem_statement'],\n",
    "                        'total_tokens': ground_truth_patch['token_count'],\n",
    "                        'total_solved': total_solved,\n",
    "                        'P': gt_patch_info\n",
    "                    }\n",
    "                    \n",
    "                    for i in range(1, max_patches + 1):\n",
    "                        row[f'P_correct{i}'] = ''\n",
    "                        row[f'P_incorrect{i}'] = ''\n",
    "                    \n",
    "                    for i, patch in enumerate(correct_combo, 1):\n",
    "                        row[f'P_correct{i}'] = patch\n",
    "                    \n",
    "                    for i, patch in enumerate(incorrect_patches, 1):\n",
    "                        row[f'P_incorrect{i}'] = patch\n",
    "                    \n",
    "                    train_rows.append(row)\n",
    "    \n",
    "    return train_rows\n",
    "\n",
    "def create_training_data(df, num_workers=None):\n",
    "    with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "        results = list(executor.map(process_instance, df.groupby('instance_id')))\n",
    "    \n",
    "    all_rows = [row for instance_rows in results for row in instance_rows]\n",
    "    return pd.DataFrame(all_rows).fillna('')\n",
    "\n",
    "# Usage example:\n",
    "train_df = create_training_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Final Manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j in train_df.iterrows():\n",
    "    st = set()\n",
    "    st.add(j['P']['patch'])\n",
    "    cnt = 1\n",
    "    for k in range(1, 7):\n",
    "        if j[f'P_correct{k}'] == '' and j[f'P_incorrect{k}'] == '':\n",
    "            continue\n",
    "        elif j[f'P_correct{k}'] == '':\n",
    "            st.add(j[f'P_incorrect{k}']['patch'])\n",
    "            cnt += 1\n",
    "        elif j[f'P_incorrect{k}'] == '':\n",
    "            st.add(j[f'P_correct{k}']['patch'])\n",
    "            cnt += 1\n",
    "    if len(st) != cnt:\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert train_df['P'].apply(lambda x: x['target'] != 1).sum() == 0\n",
    "train_df_new = train_df.copy()\n",
    "train_df_new['P_correct1'] = train_df_new['P']\n",
    "train_df = pd.concat([train_df, train_df_new], ignore_index=True)\n",
    "train_df = train_df[train_df['P_correct1'] != ''].reset_index(drop=True)\n",
    "train_df.to_csv('reward_data/train_df_augmented.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Jsonl Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('reward_data/train_df_augmented.csv')\n",
    "train_df.fillna('', inplace=True)\n",
    "def evaluate(x):\n",
    "    try:\n",
    "        return eval(x)\n",
    "    except:\n",
    "        return x\n",
    "for i in range(1, 10):\n",
    "    train_df[f'P_correct{i}'] = train_df[f'P_correct{i}'].parallel_apply(evaluate)\n",
    "    train_df[f'P_incorrect{i}'] = train_df[f'P_incorrect{i}'].parallel_apply(evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_analysis_message(bug_fixing, new_issues, patch_number):\n",
    "    if bug_fixing == 0:\n",
    "        bug_fixing_analysis = \"The changes made in this patch are incorrect and fail to address the reported issue.\"\n",
    "    elif bug_fixing == 1:\n",
    "        bug_fixing_analysis = \"The changes are partially correct. While they address the issue to some extent, they overlook critical corner cases.\"\n",
    "    elif bug_fixing == 2:\n",
    "        bug_fixing_analysis = \"The changes made in this patch are correct and comprehensively resolve the reported issue.\"\n",
    "\n",
    "    if new_issues == 0:\n",
    "        regression_bug_analysis = \"This patch introduces multiple regression bugs, potentially impacting other functionalities.\"\n",
    "    elif new_issues == 1:\n",
    "        regression_bug_analysis = \"This patch introduces a few regression bugs, which require additional attention.\"\n",
    "    elif new_issues == 2:\n",
    "        regression_bug_analysis = \"This patch does not introduce any regression bugs.\"\n",
    "\n",
    "    message = f\"\"\"\n",
    "<patch_analysis>\n",
    "  <patch_number>{patch_number}</patch_number>\n",
    "  <bug_fixing_analysis>\n",
    "    {bug_fixing_analysis}\n",
    "    <score>{bug_fixing}</score>\n",
    "  </bug_fixing_analysis>\n",
    "  <regression_risk_analysis>\n",
    "    {regression_bug_analysis}\n",
    "    <score>{new_issues}</score>\n",
    "  </regression_risk_analysis>\n",
    "</patch_analysis>\n",
    "    \"\"\"\n",
    "    return message.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_combinations(data, n):\n",
    "    size = len(data)\n",
    "    # Generate n different permutations at once using NumPy\n",
    "    perms = np.array([np.random.permutation(size) for _ in range(n)])\n",
    "    \n",
    "    # Remove any duplicate permutations if they occur\n",
    "    unique_perms = np.unique(perms, axis=0)\n",
    "    while len(unique_perms) < n:\n",
    "        new_perms = np.random.permutation(size).reshape(1, -1)\n",
    "        unique_perms = np.unique(np.vstack([unique_perms, new_perms]), axis=0)\n",
    "    \n",
    "    return [[data[i] for i in perm] for perm in unique_perms[:n]]\n",
    "\n",
    "def process_row(row_data):\n",
    "    row, idx = row_data\n",
    "    patches = []\n",
    "    for i in range(1, 6):\n",
    "        if row[f'P_correct{i}'] != '':\n",
    "            patches.append(row[f'P_correct{i}'])\n",
    "        if row[f'P_incorrect{i}'] != '':\n",
    "            patches.append(row[f'P_incorrect{i}'])\n",
    "    \n",
    "    p_count = len(patches)\n",
    "    if 3 < p_count <= 6:\n",
    "        n_samples = 2\n",
    "    else:\n",
    "        n_samples = 1\n",
    "    \n",
    "    n_combinations = get_n_combinations(patches, n_samples)\n",
    "    return [{\n",
    "        'patches': combi,\n",
    "        'problem_statement': row['problem_statement'],\n",
    "        'instance_id': row['instance_id'],\n",
    "    } for combi in n_combinations]\n",
    "\n",
    "def create_train_samples(df):\n",
    "    with Pool() as pool:\n",
    "        results = pool.map(process_row, [(row, idx) for idx, row in df.iterrows()])\n",
    "    \n",
    "    new_rows = [item for sublist in results for item in sublist]\n",
    "    return pd.DataFrame(new_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = create_train_samples(train_df)\n",
    "final_df.to_csv('reward_data/final_df.csv', index=False)\n",
    "final_df.patches.apply(len).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_needed_info(patches):\n",
    "    patch_info = []\n",
    "    for idx, patch in enumerate(patches):\n",
    "        patch_info.append({\n",
    "            'patch': patch['patch'],\n",
    "            'patch_analysis': get_analysis_message(patch['bug_fixing'], patch['new_issues'], idx + 1),\n",
    "            'target': patch['target'],\n",
    "            'critique': patch['critique'],\n",
    "        })\n",
    "    return patch_info\n",
    "\n",
    "final_df['patches'] = final_df['patches'].parallel_apply(get_needed_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j in final_df.sample(10).iterrows():\n",
    "    print(\"Status: \", j['patches'][0]['target'])\n",
    "    print(\"Critique: \", j['patches'][0]['critique'])\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('reward_data/final_df_before_user_prompt.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Test Set Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_template = '''You are an expert software engineering evaluator tasked with analyzing and selecting the best solution patch for a GitHub issue. Your goal is to thoroughly evaluate each proposed patch and determine which one most effectively solves the issue.\n",
    "\n",
    "First, examine the proposed patches:\n",
    "\n",
    "<patches>\n",
    "{patches}\n",
    "</patches>\n",
    "\n",
    "Now, carefully read the following GitHub issue:\n",
    "\n",
    "<github_issue>\n",
    "{issue}\n",
    "</github_issue>\n",
    "\n",
    "Your task is to evaluate each patch based on its effectiveness in fixing the bug. Use the following scoring system:\n",
    "\n",
    "1. Bug Fixing Score (0-2):\n",
    "   0: Incorrect changes that won't fix the issue\n",
    "   1: Partially correct changes (might fix the issue but misses corner cases)\n",
    "   2: Correct changes that will fully fix the issue\n",
    "\n",
    "2. Regression Risk (Score 0-2):\n",
    "   0: High risk of introducing multiple regression bugs\n",
    "   1: Moderate risk of introducing a few regression bugs\n",
    "   2: Low risk of introducing any regression bugs\n",
    "\n",
    "For each patch, provide a detailed analysis using the following structure:\n",
    "\n",
    "<patch_analysis>\n",
    "  <patch_number>[Patch number]</patch_number>\n",
    "  <bug_fixing_analysis>\n",
    "    [Your analysis of the bug fixing approach]\n",
    "    <score>[Score (0-2)]</score>\n",
    "  </bug_fixing_analysis>\n",
    "  <regression_risk_analysis>\n",
    "    [Your analysis of potential regression risks]\n",
    "    <score>[Score (0-2)]</score>\n",
    "  </regression_risk_analysis>\n",
    "</patch_analysis>\n",
    "\n",
    "In your analysis, consider:\n",
    "1. How well the patch addresses the core issue described in the GitHub issue?\n",
    "2. Will the patch introduce regression bugs or other issues?\n",
    "\n",
    "Please reason step by step, and put your final answer in the following format:\n",
    "\n",
    "<best_patch>Number of the best patch</best_patch>\n",
    "\n",
    "Important Tips:\n",
    "1. For each patch, write down the main changes and their potential impact\n",
    "2. Consider potential edge cases and how each patch addresses them\n",
    "3. Please do not prioritize one patch over another based on the position in the list.\n",
    "'''\n",
    "\n",
    "solution_template = '''<think>\n",
    "Here is the analysis of the proposed solution patches:\n",
    "\n",
    "<analysis>\n",
    "{analysis}\n",
    "</analysis>\n",
    "\n",
    "Based on the analysis, here is the evaluation for each patch:\n",
    "\n",
    "<scoring>\n",
    "{scoring}\n",
    "</scoring>\n",
    "\n",
    "Based on the scoring system, the best solution patch is:\n",
    "\n",
    "<best_patch>{best_patch_number}</best_patch>\n",
    "\n",
    "</think>\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_analysis_message(bug_fixing, new_issues, patch_number):\n",
    "    if bug_fixing == 0:\n",
    "        bug_fixing_analysis = \"The changes made in this patch are incorrect and fail to address the reported issue.\"\n",
    "    elif bug_fixing == 1:\n",
    "        bug_fixing_analysis = \"The changes are partially correct. While they address the issue to some extent, they overlook critical corner cases.\"\n",
    "    elif bug_fixing == 2:\n",
    "        bug_fixing_analysis = \"The changes made in this patch are correct and comprehensively resolve the reported issue.\"\n",
    "\n",
    "    if new_issues == 0:\n",
    "        regression_bug_analysis = \"This patch introduces multiple regression bugs, potentially impacting other functionalities.\"\n",
    "    elif new_issues == 1:\n",
    "        regression_bug_analysis = \"This patch introduces a few regression bugs, which require additional attention.\"\n",
    "    elif new_issues == 2:\n",
    "        regression_bug_analysis = \"This patch does not introduce any regression bugs.\"\n",
    "\n",
    "    message = f\"\"\"\n",
    "<patch_analysis>\n",
    "  <patch_number>{patch_number}</patch_number>\n",
    "  <bug_fixing_analysis>\n",
    "    {bug_fixing_analysis}\n",
    "    <score>{bug_fixing}</score>\n",
    "  </bug_fixing_analysis>\n",
    "  <regression_risk_analysis>\n",
    "    {regression_bug_analysis}\n",
    "    <score>{new_issues}</score>\n",
    "  </regression_risk_analysis>\n",
    "</patch_analysis>\n",
    "    \"\"\"\n",
    "    return message.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('reward_data/final_df_before_user_prompt.csv')\n",
    "df['patches'] = df['patches'].parallel_apply(eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_patch_context(patch):\n",
    "    while patch[-1] == '\\n':\n",
    "        patch = patch[:-1]\n",
    "    return f'\\n<patch>\\n{patch}\\n<\\patch>\\n'\n",
    "\n",
    "def get_user_prompt(patches, issue):\n",
    "    all_patches = [handle_patch_context(patch['patch']) for patch in patches]\n",
    "    \n",
    "    patches_str = \"\\n\".join([f\"Solution Patch {idx + 1}: {patch}\" for idx, patch in enumerate(all_patches)])\n",
    "    return user_template.format(patches=patches_str, issue=issue)\n",
    "\n",
    "def get_solution_prompt(patches):\n",
    "    scoring = \"\\n\".join([patch['patch_analysis'] for patch in patches])\n",
    "    solved_idx = -1\n",
    "    for idx, patch in enumerate(patches):\n",
    "        if patch['target'] == 1:\n",
    "            solved_idx = idx + 1\n",
    "            break\n",
    "    analysis = \"\\n\".join([f\"Solution Patch {idx + 1}: {patch['critique']}\" for idx, patch in enumerate(patches)])\n",
    "    return solution_template.format(analysis=analysis, best_patch_number=solved_idx, scoring=scoring)\n",
    "df['user_prompt'] = df.parallel_apply(lambda x: get_user_prompt(x['patches'], x['problem_statement']), axis=1)\n",
    "df['solution_prompt'] = df['patches'].parallel_apply(get_solution_prompt)\n",
    "\n",
    "for i, j in df.sample(2).iterrows():\n",
    "    print(j['solution_prompt'])\n",
    "    print('=' * 100)\n",
    "\n",
    "df.to_csv('reward_data/final_df_user_prompt.csv', index=False)\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_row_to_chat_format(row):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": row['user_prompt']},\n",
    "            {\"role\": \"assistant\", \"content\": row['solution_prompt']}\n",
    "        ],\n",
    "        \"format\": \"chatml\"\n",
    "    }\n",
    "\n",
    "df['chat_data'] = df.parallel_apply(convert_row_to_chat_format, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list = df['chat_data'].tolist()\n",
    "with open('reward_data/train.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for item in final_list:\n",
    "        f.write(json.dumps(item) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swe-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
